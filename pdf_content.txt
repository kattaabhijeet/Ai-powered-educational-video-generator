Visual
 
Learning
 
Pattern
 
Analysis
 
+
 
AI
 
Video
 
Synthesis
 
System
 
Video
 
Link:
 
->
 
 
How Web Sockets work | Deep Dive
Objective
 
Create
 
a
 
hybrid
 
system
 
where:
 
●
 
Manual
 
Step:
 
The
 
Engineer
 
watches
 
the
 
reference
 
video
 
and
 
identifies
 
the
 
visualization
 
style.
 
 
●
 
Automatic
 
Step:
 
AI
 
generates
 
a
 
new
 
MP4
 
video
 
in
 
that
 
style
 
for
 
any
 
topic.
 
 
 
PART
 
A
 
—
 
Manual
 
Research
 
Task
 
1.
 
Analyze
 
the
 
reference
 
video
 
(Manual)
 
The
 
engineer
 
must
 
watch
 
the
 
video
 
link
 
provided
 
and
 
manually
 
document:
 
Visualization
 
Style
 
●
 
2D
 
explainer
 
●
 
line-based
 
animation
 
●
 
flowchart
 
+
 
arrow
 
animations
 
●
 
character-based
 
●
 
whiteboard/doodle
 
●
 
UI
 
walkthrough
 
●
 
kinetic
 
typography
 
●
 
infographic
 
motion
 
graphics
 
●
 
storytelling
 
scenes
 
 
This
 
becomes
 
the
 
foundation
 
for
 
the
 
AI.
 
Create
 
an
 
Goggle
 
docs
 
for
 
report
 
generation.
  
PART
 
B
 
—
 
Automatic
 
System
 
(Prototype)
 
2.
 
Topic
 
→
 
Script
 
(AI
 
Automatic)
 
Given
 
any
 
topic,
 
AI
 
automatically
 
generates:
 
●
 
narration
 
script
 
●
 
scene-by-scene
 
concepts
 
●
 
short
 
explanations
 
●
 
voice-over
 
text
 
3.
 
Script
 
→
 
Animation
 
Blueprint
 
(AI
 
Automatic)
 
AI
 
converts
 
the
 
script
 
+
 
manual
 
style_profile
 
into:
 
●
 
storyboard
 
●
 
element
 
list
 
●
 
animation
 
instructions
 
●
 
timing
 
●
 
transitions
 
●
 
prompts
 
for
 
asset
 
generation
 
 
4.
 
Blueprint
 
→
 
MP4
 
Video
 
(AI
 
Automatic)
 
The
 
prototype
 
should
 
automatically
 
generate
 
a
 
video
 
by:
 
Tools
 
allowed
 
●
 
Lottie
 
 
●
 
Manim
 
●
 
FFMPEG
 
●
 
Pika
 
/
 
Runway
 
(API-based
 
video
 
gen)
 
●
 
Any
 
internal
 
or
 
open-source
 
animation
 
framework
 
 
The
 
MP4
 
must
 
include:
 Visuals
 
based
 
on
 
the
 
blueprint
 
 
Text
 
overlays
 
 
Basic
 
transitions
 
 
Optional
 
narration
 
(AI
 
TTS)
 
 